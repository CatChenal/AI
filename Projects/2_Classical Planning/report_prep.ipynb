{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_First code cell from custom template `dsml.ipynb` in [user_path]\\Anaconda3\\envs\\dsml\\Lib\\site-packages\\jupyterlab_templates\\templates\\jupyterlab_templates_. <br>\n",
    "_See Tim Paine's [`jupyter_lab templates` extention](https://github.com/timkpaine/jupyterlab_templates)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path, PurePath as PPath\n",
    "\n",
    "print('Python ver: {}\\nPython env: {}'.format(sys.version, Path(sys.prefix).name))\n",
    "print('Currrent dir: {}\\n'.format(Path.cwd()))\n",
    "\n",
    "def add_to_sys_path(this_path, up=False):\n",
    "    \"\"\"\n",
    "    Prepend this_path to sys.path.\n",
    "    If up=True, path refers to parent folder (1 level up).\n",
    "    \"\"\"\n",
    "    if up:\n",
    "        # NB: Path does not have a str method.\n",
    "        newp = str(PPath(this_path).parent)\n",
    "    else:\n",
    "        newp = str(PPath(this_path)) \n",
    "    \n",
    "    if newp not in sys.path:\n",
    "        sys.path.insert(1, newp)\n",
    "        print('Path added to sys.path: {}'.format(newp))\n",
    "\n",
    "# if notebook inside another folder, eg ./notebooks:\n",
    "nb_folder = 'notebooks'\n",
    "add_to_sys_path(Path.cwd(), Path.cwd().name.startswith(nb_folder))\n",
    "\n",
    "\n",
    "def get_project_dirs(which=['data', 'images'], nb_folder='notebooks'):\n",
    "    dir_lst = []\n",
    "    if Path.cwd().name.startswith(nb_folder):\n",
    "        dir_fn = Path.cwd().parent.joinpath\n",
    "    else:\n",
    "        dir_fn = Path.cwd().joinpath\n",
    "        \n",
    "    for d in which:\n",
    "        DIR = dir_fn(d)\n",
    "        if not DIR.exists():\n",
    "            Path.mkdir(DIR)\n",
    "        dir_lst.append(DIR)\n",
    "    return dir_lst\n",
    "\n",
    "DIR_DATA, DIR_IMG = get_project_dirs()\n",
    "    \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats as sps\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pandas as pd\n",
    "#pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "plt.ion()\n",
    "plt.style.use('seaborn-muted')\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Filtered dir() for method discovery:\n",
    "def filter_dir(obj, start_with_str='_', exclude=True):\n",
    "    return [d for d in dir(obj) if not d.startswith(start_with_str) == exclude]\n",
    "\n",
    "def get_mdl_pkgs(alib):\n",
    "    import inspect\n",
    "    \"Inspect module hierarchy on two levels ony.\"\n",
    "    for name, mdl in inspect.getmembers(alib, inspect.ismodule):\n",
    "        print('\\n{:>13} : {}'.format(mdl.__name__, filter_dir(mdl)))\n",
    "        for mdl_name, mdl_sub in inspect.getmembers(mdl, inspect.ismodule):\n",
    "            if mdl_sub.__doc__:\n",
    "                print('\\n{:>20} : {}'.format(mdl_name, mdl_sub.__doc__.strip()))\n",
    "                \n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from IPython import get_ipython\n",
    "from IPython.display import HTML, Markdown, display #, IFrame\n",
    "# for presentations:\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "def new_section(title='New section'):\n",
    "    style = \"text-align:center;background:#c2d3ef;padding:16px;color:#ffffff;font-size:2em;width:98%\"\n",
    "    return HTML('<div style=\"{}\">{}</div>'.format(style, title))\n",
    "\n",
    "\n",
    "def add_div(div_class, div_start, div_text, output_string=True):\n",
    "    \"\"\"\n",
    "    Behaviour with default `output_string=True`:\n",
    "    The cell is overwritten with the output string, but the cell mode is still in 'code' not 'markdown':\n",
    "    ```\n",
    "    [x]\n",
    "    add_div('alert-warning', 'Tip: ', 'some tip here', output_string=True)\n",
    "    [x]\n",
    "    <div class=\"alert alert-warning\"><b>Tip: </b>some tip here</div>\n",
    "    ```\n",
    "    The only thing to do is change the cell mode to Markdown.\n",
    "    If `output_string=False`, the HTML output is displayed in an output cell.\n",
    "    \"\"\"\n",
    "    accepted = ['info', 'warning', 'danger']\n",
    "    if div_class not in accepted:\n",
    "        return HTML(f\"\"\"<div class=\"alert\"><b>Wrong class:</b> `div_start` is one of {accepted}.\n",
    "                    </div>\"\"\")\n",
    "    div = f\"\"\"<div class=\"alert alert-{div_class}\"><b>{div_start}</b><br>{div_text}</div>\"\"\"\n",
    "    if output_string:\n",
    "        return get_ipython().set_next_input(div, 'markdown')\n",
    "    else:\n",
    "        return Markdown(div) #HTML(div)\n",
    "    \n",
    "    \n",
    "def add_div_around_html(div_html_text, output_string=True, div_style=\"{width: 80%}\"):\n",
    "    \"\"\"\n",
    "    Wrap an html code str inside a div.\n",
    "    div_style: whatever follows style= within the <div>\n",
    "    \n",
    "    Behaviour with default `output_string=True`:\n",
    "    The cell is overwritten with the output string (but the cell mode is still in 'code' not 'markdown')\n",
    "    The only thing to do is change the cell mode to Markdown.\n",
    "    If `output_string=False`, the HTML output is displayed in an output cell.\n",
    "    \"\"\"\n",
    "    div = f\"\"\"<div style={div_style}>{div_html_text}</div>\"\"\"\n",
    "    if output_string:\n",
    "        return get_ipython().set_next_input(div, 'markdown')\n",
    "    else:\n",
    "        return HTML(div)\n",
    "    \n",
    "# autoreload extension\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "\n",
    "if 'autoreload' not in ipython.extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report preparation - Udacity AI Project 2 Classical Planning Agent\n",
    "\n",
    "* Process raw data obtained from modified `run_search.py` (`run_report.py`) into pandas DataFrames\n",
    "* Create charts and analyses as per requirements\n",
    "* Produce a html report printed as pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### The complete specifications of the problem can be found [in the Udacity AIND repo](https://github.com/udacity/artificial-intelligence/tree/master/Projects/2_Classical%20Planning). \n",
    "\n",
    "- [x] 1. Run example: `>python example_have_cake.py` in workspace\n",
    "- [x] 2. Complete the TODO sections in `my_planning_graph.py` and implement the following functions (methods):  \n",
    " - [x] 2.1 `ActionLayer._inconsistent_effects`\n",
    " - [x] 2.2 `ActionLayer._interference`\n",
    " - [x] 2.3 `ActionLayer._competing_needs`\n",
    " - [x] 2.4 `LiteralLayer._inconsistent_support`\n",
    " - [x] 2.6 `LiteralLayer._negation`\n",
    " - [x] 2.7 `PlanningGraph.h_levelsum`\n",
    " - [x] 2.7 `PlanningGraph.h_maxlevel`  \n",
    " - [x] 2.7 `PlanningGraph.h_setlevel`  \n",
    " - [x] 2.8  Test solution by running `python -m unittest -v`\n",
    "\n",
    "- [x] 3. Experiment with different search algorithms using `run_search.py`. \n",
    "  > The goal of your experiment is to understand the tradeoffs in speed, optimality, and complexity of progression search as problem size increases. \n",
    " You can also run specific problems & search algorithms - e.g., to run breadth first search and UCS on problems 1 and 2: `python run_search.py -p 1 2 -s 1 2`\n",
    "\n",
    "- [x] 4. Experiment with the planning algorithms\n",
    "  > The run_search.py script allows you to choose any combination of 11 search algorithms (3 uninformed and 8 with heuristics) on 4 air cargo problems.\n",
    "\n",
    " - [x] 4.1 You should run all of the search algorithms on the first 2 problems and record the following information for each combination:\n",
    "  - number of actions in the domain\n",
    "  - number of new node expansions\n",
    "  - time to complete the plan search  \n",
    "  \n",
    "- [x] 5. Use the results from the first 2 problems to determine whether _any of the **uninformed search** algorithms_ should be excluded for problems 3 and 4. \n",
    "  > You must run at least 1 uninformed search, 2 heuristics with greedy best first search, and 2 heuristics with A* on problems 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Report Requirements\n",
    "\n",
    "Your submission for review **must** include a report named \"report.pdf\" that includes all of the figures (charts or tables) and written responses to the questions below. You may plot multiple results for the same topic on the same chart or use multiple charts. (Hint: you may see more detail by using log space for one or more dimensions of these charts.)\n",
    "\n",
    "- [x] Use a table or chart to analyze the number of nodes expanded against number of actions in the domain\n",
    "- [x] Use a table or chart to analyze the search time against the number of actions in the domain\n",
    "- [ ] Use a table or chart to analyze the length of the plans returned by each algorithm on all search problems\n",
    "\n",
    "Use your results to answer these questions:\n",
    "- Question 1: Which algorithm or algorithms would be most appropriate for planning in a very restricted domain (i.e., one that has only a few actions) and needs to operate in real time? \n",
    "- Question 2: Which algorithm or algorithms would be most appropriate for planning in very large domains (e.g., planning delivery routes for all UPS drivers in the U.S. on a given day)\n",
    "- Question 3: Which algorithm or algorithms would be most appropriate for planning problems where it is important to find only optimal plans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Report evaluation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style = \"width:80%\">\n",
    "<table class=\"table table-bordered section-table\"> <thead> <tr> <!-- ngIf: !ctrl.hideCriteria --><th class=\"rubric-category criteria col-xs-3 ng-scope\" ng-if=\"!ctrl.hideCriteria\"> <span translate=\"\" class=\"ng-scope\">Criteria</span> </th><!-- end ngIf: !ctrl.hideCriteria --> <th class=\"rubric-category meets-specs\" ng-class=\"ctrl.reviewerTips ? col-xs-7 : col-xs-4\"> <span translate=\"\" class=\"ng-scope\">Meets Specifications</span> </th> <!-- ngIf: ctrl.reviewerTips --> </tr> </thead> <tbody>  <!-- ngRepeat: rubricItem in section.rubric_items --><tr ng-repeat=\"rubricItem in section.rubric_items\" class=\"ng-scope\"> <!-- ngIf: !ctrl.hideCriteria --><td class=\"rubric-item col-xs-3 ng-binding ng-scope\" ng-if=\"!ctrl.hideCriteria\" ng-bind-html=\"localize(rubricItem, 'criteria', markup=true)\"><p>Analyze the search complexity as a function of domain size, search algorithm, and heuristic.</p>\n",
    "</td><!-- end ngIf: !ctrl.hideCriteria --> <td class=\"rubric-item ng-binding\" ng-class=\"ctrl.reviewerTips ? col-xs-7 : col-xs-4\" ng-bind-html=\"localize(rubricItem, 'passed_description', markup=true)\"><p>Report includes a table or chart to analyze the number of nodes expanded against number of actions in the domain.</p>\n",
    "<ul>\n",
    "<li>The chart or table includes data for all search &amp; heuristic combinations for air cargo problems 1 and 2</li>\n",
    "<li>The chart or table includes data <strong>at least</strong> one uninformed search, two heuristics with greedy best first search, and two heuristics with A* on air cargo problems 3 and 4</li>\n",
    "<li>Report includes at least a one paragraph discussion of these results that analyzes the growth trends as the problem size increases</li>\n",
    "</ul>\n",
    "</td> <!-- ngIf: ctrl.reviewerTips --> </tr><!-- end ngRepeat: rubricItem in section.rubric_items --><tr ng-repeat=\"rubricItem in section.rubric_items\" class=\"ng-scope\"> <!-- ngIf: !ctrl.hideCriteria --><td class=\"rubric-item col-xs-3 ng-binding ng-scope\" ng-if=\"!ctrl.hideCriteria\" ng-bind-html=\"localize(rubricItem, 'criteria', markup=true)\"><p>Analyze search time as a function of domain size, search algorithm, and heuristic.</p>\n",
    "</td><!-- end ngIf: !ctrl.hideCriteria --> <td class=\"rubric-item ng-binding\" ng-class=\"ctrl.reviewerTips ? col-xs-7 : col-xs-4\" ng-bind-html=\"localize(rubricItem, 'passed_description', markup=true)\"><p>Report includes a table or chart to analyze the search time against the number of actions in the domain.</p>\n",
    "<ul>\n",
    "<li>The chart or table includes data for all search &amp; heuristic combinations for air cargo problems 1 and 2</li>\n",
    "<li>The chart or table includes data <strong>at least</strong> one uninformed search, two heuristics with greedy best first search, and two heuristics with A* on air cargo problems 3 and 4</li>\n",
    "<li>Report includes at least a one paragraph discussion of these results that analyzes the growth trends as the problem size increases</li>\n",
    "</ul>\n",
    "</td> <!-- ngIf: ctrl.reviewerTips --> </tr><!-- end ngRepeat: rubricItem in section.rubric_items --><tr ng-repeat=\"rubricItem in section.rubric_items\" class=\"ng-scope\"> <!-- ngIf: !ctrl.hideCriteria --><td class=\"rubric-item col-xs-3 ng-binding ng-scope\" ng-if=\"!ctrl.hideCriteria\" ng-bind-html=\"localize(rubricItem, 'criteria', markup=true)\"><p>Analyze the optimality of solution as a function of domain size, search algorithm, and heuristic.</p>\n",
    "</td><!-- end ngIf: !ctrl.hideCriteria --> <td class=\"rubric-item ng-binding\" ng-class=\"ctrl.reviewerTips ? col-xs-7 : col-xs-4\" ng-bind-html=\"localize(rubricItem, 'passed_description', markup=true)\"><p>Report includes a table or chart to analyze the length of the plans returned by each algorithm on all search problems.</p>\n",
    "<ul>\n",
    "<li>The chart or table includes data for all search &amp; heuristic combinations for air cargo problems 1 and 2</li>\n",
    "<li>The chart or table includes data <strong>at least</strong> one uninformed search, two heuristics with greedy best first search, and two heuristics with A* on air cargo problems 3 and 4</li>\n",
    "</ul>\n",
    "</td> <!-- ngIf: ctrl.reviewerTips --> </tr><!-- end ngRepeat: rubricItem in section.rubric_items --><tr ng-repeat=\"rubricItem in section.rubric_items\" class=\"ng-scope\"> <!-- ngIf: !ctrl.hideCriteria --><td class=\"rubric-item col-xs-3 ng-binding ng-scope\" ng-if=\"!ctrl.hideCriteria\" ng-bind-html=\"localize(rubricItem, 'criteria', markup=true)\"><p>Report answers all required questions</p>\n",
    "</td><!-- end ngIf: !ctrl.hideCriteria --> <td class=\"rubric-item ng-binding\" ng-class=\"ctrl.reviewerTips ? col-xs-7 : col-xs-4\" ng-bind-html=\"localize(rubricItem, 'passed_description', markup=true)\"><p>Submission includes a short answer to each of the following questions. (A short answer should be at least 1-2 sentences at most a small paragraph.)</p>\n",
    "<ul>\n",
    "<li>Which algorithm or algorithms would be most appropriate for planning in a very restricted domain (i.e., one that has only a few actions) and needs to operate in real time?</li>\n",
    "<li>Which algorithm or algorithms would be most appropriate for planning in very large domains (e.g., planning delivery routes for all UPS drivers in the U.S. on a given day)</li>\n",
    "<li>Which algorithm or algorithms would be most appropriate for planning problems where it is important to find only optimal plans?</li>\n",
    "</ul>\n",
    "</td> <!-- ngIf: ctrl.reviewerTips --> </tr><!-- end ngRepeat: rubricItem in section.rubric_items --> </tbody> </table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Report prep \n",
    "\n",
    "I modified `run_search.main()` and `_utils.run_search` to obtain a tab separated file to facilitate the processing of the results.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data subfolders for raw and processed files:\n",
    "DATA_DIRS = []\n",
    "for fldr in ['raw', 'processed']:\n",
    "    p = DIR_DATA.joinpath(fldr)\n",
    "    if not p.exists():\n",
    "        Path.mkdir(p)\n",
    "    DATA_DIRS.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial results : all searches on problems 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import report as rpt\n",
    "from display_figure import display_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCHES = rpt.SEARCHES\n",
    "SEARCHES\n",
    "    \n",
    "# Problem names    \n",
    "problems = rpt.problems\n",
    "\n",
    "#Problem specifications (df)\n",
    "specs = rpt.specs #rpt.get_prob_specs()\n",
    "specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data for p1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File used if replace=False: DATA_DIRS[1].joinpath('prob_1_df.csv')\n",
    "\n",
    "df1 = rpt.get_problem_data_df('prob_1', problems[0], DATA_DIRS[0], DATA_DIRS[1], file_as_tsv=True, replace=False)\n",
    "df1.shape\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data for p2:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File used if replace=False: DATA_DIRS[1].joinpath('prob_2_df.csv')\n",
    "\n",
    "df2 = rpt.get_problem_data_df('prob_2', problems[1], DATA_DIRS[0], DATA_DIRS[1], file_as_tsv=True, replace=False)\n",
    "df2.shape\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chart creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = DIR_IMG.joinpath('p12_NewNodes_Actions.png')\n",
    "fig2 = DIR_IMG.joinpath('p12_ElapsedSeconds_Actions.png')\n",
    "\n",
    "done = True\n",
    "\n",
    "if not done:\n",
    "    # First plot: 'NewNodes' vs 'Actions'\n",
    "    rpt.make_bar_plots([df1, df2],\n",
    "                   'Actions', 'NewNodes',\n",
    "                   [problems[0], problems[1]],\n",
    "                   legend_bbox=(1.05, .95),\n",
    "                   to_file=fig1)\n",
    "\n",
    "    # Second plot: 'ElapsedSeconds' vs 'Actions'\n",
    "    rpt.make_bar_plots([df1, df2],\n",
    "                   'Actions', 'ElapsedSeconds',\n",
    "                   [problems[0], problems[1]],\n",
    "                   legend_bbox=(1.05, .95),\n",
    "                   to_file=fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis for Problems 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specs_ml = specs[specs.columns[1:]].to_html(index=False, justify='center')\n",
    "replace_str1 = ' style=\"text-align: center;\"'\n",
    "replace_str2 = 'class=\"dataframe\"'\n",
    "specs_ml = specs_ml.replace(replace_str1, '')\n",
    "specs_ml = specs_ml.replace(replace_str2, replace_str1)\n",
    "\n",
    "analysis = f\"\"\"\n",
    "    <h1> Analysis of the output of Problems 1 and 2</h1>\n",
    "    <p style=\"font-size:110%;\">\n",
    "        All four problems differ by the number of nodes, cargo loads, airports, and specificity of goals. \n",
    "        <pre>\n",
    "            {specs_ml}\n",
    "        </pre>\n",
    "    </p>\n",
    "    <p style=\"font-size:110%;\">\n",
    "        The first two problems are the less complex and are used to decide on the appropriate choices of search \n",
    "        functions and heuristic on more \"real life\" problems (Problems 3 and 4).<br>\n",
    "        The analysis relies on three charts:\n",
    "        <ul>\n",
    "            <li> Number of nodes expanded against number of actions in the domain</li>\n",
    "            <li> Search time against the number of actions in the domain</li>\n",
    "            <li> Length of the plans returned by each algorithm on all search problems</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt.add_div_around_html(analysis, output_string=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dict = {'div': {'width': 7},\n",
    "              'figure': {'display':'inline-block', 'text-align':'left'},\n",
    "              'image': {'display':'block', 'width': 600, 'height':500},\n",
    "              'caption': {'color':'teal','font-weight':'bold', 'font-family': 'Arial, Helvetica, sans-serif'}\n",
    "             }\n",
    "\n",
    "caption1_dict = {'number': 1,\n",
    "                'caption': 'NewNodes expanded in each search function for Problems 1 and 2.&nbsp;&nbsp;(Note the log scale for Problem 2.)'}\n",
    "\n",
    "caption2_dict = {'number': 2,\n",
    "                'caption': 'ElapsedSeconds in each search function for Problems 1 and 2.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_figure(fig1, style_dict, caption1_dict, img_title='NewNodes', return_html=True)\n",
    "display_figure(fig2, style_dict, caption2_dict, img_title='Seconds', return_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = rpt.get_elim_candidates(df2, df1)\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insights = rpt.paragraph_p12(candidates, return_html=True)\n",
    "#rpt.add_div_around_html(insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt.add_div_around_html(insights, output_string=False) #output_string=True in final rpt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "candidates = rpt.get_elim_candidates(df2, df1)\n",
    "insights = rpt.paragraph_p12(candidates, return_html=True)\n",
    "rpt.add_div_around_html(insights, output_string=False) #output_string=True in final rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Problems 3 & 4\n",
    "## Intro, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p34_intro = \"\"\"\n",
    "<h2>Air cargo problems 3 and 4:</h2>\n",
    "   <p style=\"font-size:110%;\">\n",
    "    After elimination of problems 3, 8, and 10, the searches performed on both Problems 3 & 4 were the following<br>\n",
    "    (with the number corresponds to the number passed to `run_report.py` in the -s argument):\n",
    "    </p>\n",
    "    <pre> \n",
    "      <b>Uninformed searches </b>\n",
    "        1: breadth_first_search\n",
    "        2: depth_first_graph_search\n",
    "      <b>Informed searches with two different heuristics </b>\n",
    "        4: greedy_best_first_graph_search + h_unmet_goals\n",
    "        5: greedy_best_first_graph_search + h_pg_levelsum\n",
    "        6: greedy_best_first_graph_search + h_pg_maxlevel\n",
    "        7: greedy_best_first_graph_search + h_pg_setlevel\n",
    "        9: astar_search + h_pg_levelsum\n",
    "        11: <mark>astar_search + h_pg_setlevel</mark>\n",
    "    </pre>\n",
    "    <p style=\"font-size:110%;\">\n",
    "      Note: Search <mark>11</mark> was aborted on both problems due to excessive run time (> 1 hour).<br>\n",
    "      The raw data files were generated at the comman line using the `run_report.py` module, \n",
    "      which is a copy of the original `run_search.py` module with modifications on output string formats. \n",
    "      Note that `_utils.py.run_search` was amended accordingly.\n",
    "    </p>\n",
    "\"\"\" \n",
    "rpt.add_div_around_html(p34_intro, output_string=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p3_out = DIR_DATA.joinpath('prob_3_df.csv')\n",
    "\n",
    "df3 = rpt.get_problem_data_df('prob_3', problems[2], DATA_DIRS[0], DATA_DIRS[1], file_as_tsv=True, replace=False)\n",
    "df3.shape\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p4_out = DIR_DATA.joinpath('prob_4_df.csv')\n",
    "\n",
    "df4 = rpt.get_problem_data_df('prob_4', problems[3], DATA_DIRS[0], DATA_DIRS[1], file_as_tsv=True, replace=False)\n",
    "df4.shape\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Problems 3 & 4\n",
    "## Chart creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = DIR_IMG.joinpath('p34_NewNodes_Actions.png')\n",
    "fig4 = DIR_IMG.joinpath('p34_ElapsedSeconds_Actions.png')\n",
    "\n",
    "done = True\n",
    "\n",
    "if not done:\n",
    "    # First plot: 'NewNodes' vs 'Actions'\n",
    "    rpt.make_bar_plots([df3, df4],\n",
    "                       'Actions', 'NewNodes',\n",
    "                       [problems[2], problems[3]],\n",
    "                       legend_bbox=(1.05, .95),\n",
    "                       to_file=fig3, \n",
    "                       excluded=candidates)\n",
    "\n",
    "    # Second plot: 'ElapsedSeconds' vs 'Actions'\n",
    "    rpt.make_bar_plots([df3, df4],\n",
    "                       'Actions', 'ElapsedSeconds',\n",
    "                       [problems[2], problems[3]],\n",
    "                       legend_bbox=(1.05, .95),\n",
    "                       to_file=fig4,\n",
    "                       excluded=candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_dict = {'div': {'width': 7},\n",
    "              'figure': {'display':'inline-block', 'text-align':'left'},\n",
    "              'image': {'display':'block', 'width': 600, 'height':500},\n",
    "              'caption': {'color':'teal','font-weight':'bold', 'font-family': 'Arial, Helvetica, sans-serif'}\n",
    "             }\n",
    "\n",
    "cap3 = \"\"\"NewNodes expanded in each search function for Problems 3 and 4.<br>\n",
    "<pre>         <mark>Search 11, A* with setlevel heuristic</mark>: aborted after 1 hour.<pre>\"\"\"\n",
    "caption3_dict = {'number': 3, 'caption': cap3}\n",
    "\n",
    "cap4 = \"\"\"Elapsed minutes in each search function for Problems 1 and 2.<br>\n",
    "<pre>         <mark>Search 11, A* with setlevel heuristic</mark>: aborted after 1 hour.<pre>\"\"\"\n",
    "caption4_dict = {'number': 4, 'caption': cap4}\n",
    "\n",
    "\n",
    "display_figure(fig3, style_dict, caption3_dict, img_title='NewNodes')\n",
    "display_figure(fig4, style_dict, caption4_dict, img_title='Minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# All problems: PlanLength analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use a table or chart to analyze the length of the plans returned by each algorithm on all search problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = rpt.concat_all_dfs([df1, df2, df3, df4])\n",
    "dfa_rows = dfa.shape[0]\n",
    "dfa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = rpt.concat_all_dfs([df1, df2, df3, df4])\n",
    "\n",
    "dbl_tbl, dbl_para, df_dbl = rpt.plans_length(dfa, 'double')\n",
    "sgl_tbl, sgl_para, df_sgl = rpt.plans_length(dfa, 'single')\n",
    "\n",
    "rpt.add_div_around_html(dbl_para, output_string=False)\n",
    "rpt.add_div_around_html(sgl_para, output_string=False)\n",
    "\n",
    "tbls_html = \"\"\"\n",
    "<div style=\"float:left; width:100%; padding:1px;\">\n",
    "  <div style=\"float:left; width:50%; padding:2px;\">\n",
    "    {}\n",
    "   </div>\n",
    "  <div style=\"float:left; width:50%; padding:2px;\">\n",
    "    {}\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "tables_in_row = tbls_html.format(dbl_tbl, sgl_tbl)\n",
    "\n",
    "#rpt.add_div_around_html(tables_in_row, output_string=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A1\n",
    "A1 = dfa[(dfa.ElapsedSeconds) < 1 & (dfa.Actions==20)].sort_values('ElapsedSeconds', ascending=True)\n",
    "A1_searchers = sorted(A1.id.unique())\n",
    "A1_searchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2\n",
    "A2 = dfa[(dfa.ElapsedSeconds) < 1 & (dfa.Actions > 100)].sort_values('ElapsedSeconds', ascending=True)\n",
    "A2_searchers = sorted(A2.id.unique())\n",
    "A2_searchers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>* Anwers to questions</h3>\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>Question 1:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "Which algorithm(s) would be most appropriate for planning in a very restricted domain\n",
    "(i.e., one that has only a few actions) and needs to operate in real time?\n",
    "</p>\n",
    "</blockquote>\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>&#187; Answer:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "A domain with few actions would be like Problem 1. All search algorithms would be appropriate in this case as they all performed under 1 second.\n",
    "</p>\n",
    "</blockquote>\n",
    "\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>Question 2:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "Which algorithm or algorithms would be most appropriate for planning in very large domains \n",
    "(e.g., planning delivery routes for all UPS drivers in the U.S. on a given day.)\n",
    "</p>\n",
    "</blockquote>\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>&#187; Answer:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "For problems with more than 100 and with a search time restricted to under 1 second, algorithm <b>4</b>: <i>greedy_best_first_graph_search h_unmet_goals</i> would be the best.\n",
    "</p>\n",
    "</blockquote>\n",
    "\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>Question 3:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "Which algorithm(s) would be most appropriate for planning problems where it is important to find only optimal plans?\n",
    "</p>\n",
    "</blockquote>\n",
    "<blockquote class=\"w3-panel w3-leftbar w3-light-grey\">\n",
    "<strong>&#187; Answer:</strong>\n",
    "<p style=\"font-size:110%;\">\n",
    "The algorithms that guarantee to find the shortest path and are thus optimal are: <b>1</b>: <i>breadth_first_search</i> and <b>3</b>: <i>uniform_cost_search</i>.\n",
    "</p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Plan length analysis :  \"manual code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sgle_digit_plans = dfa[dfa.PlanLength <= 9].sort_values(['PlanLength'], ascending=False) before function\n",
    "\n",
    "sgle_digit_plans = df_sgl\n",
    "sgle_uniq_probs =sgle_digit_plans['Air cargo problem'].unique()\n",
    "sd_plans = sgle_digit_plans.shape[0]\n",
    "sd_searcher_cnt = sgle_digit_plans['Searcher'].value_counts()\n",
    "sd_fn_cnt = sgle_digit_plans['search_fn'].value_counts()\n",
    "\n",
    "sgle_digit_plans\n",
    "sgle_digit_plans.sort_values('ElapsedSeconds', ascending=True).head()\n",
    "\n",
    "sf_fn = sd_fn_cnt.to_frame()\n",
    "sf_fn.reset_index(drop=False, inplace=True)\n",
    "sf_fn.columns = ['Search function','Frequency where PlanLength<10'] \n",
    "sf_fn_ml = sf_fn.to_html(index=False, justify='center')\n",
    "\n",
    "replace_str1 = ' style=\"text-align: center;\"'\n",
    "replace_str2 = 'class=\"dataframe\"'\n",
    "sf_fn_ml = sf_fn_ml.replace(replace_str1, '')\n",
    "sf_fn_ml = sf_fn_ml.replace(replace_str2, replace_str1)\n",
    "rpt.add_div_around_html(sf_fn_ml, output_string=False)\n",
    "\n",
    "pct_sd = sd_plans/dfa_rows\n",
    "top2_fn = sd_fn_cnt[0:2].sum()\n",
    "pct_top2_fn = top2_fn/sd_plans\n",
    "\n",
    "text = f\"Out of {dfa_rows} completed searches, {pct_sd:.0%} ({sd_plans}), have single-digit PlanLength.<br>\"\n",
    "text += f\"In that subset, {top2_fn:d} ({pct_top2_fn:.0%}) involve the search functions `{sd_fn_cnt.index[0]}` and `{sd_fn_cnt.index[1]}`.\"\n",
    " \n",
    "if len(sgle_uniq_probs) < 4:\n",
    "    text += \" And this occurs only for Problems: \"\n",
    "    pro = \",\".join('{}' for p in sgle_uniq_probs) +'.<br>'\n",
    "    text += pro.format(*sgle_uniq_probs)\n",
    "else:\n",
    "    text += \" And this occurs for all Problems.\"\n",
    "    \n",
    "Markdown(text)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def plans_length(dfa, which):\n",
    "    \"\"\"\n",
    "    dfa: frame of concatenated df1 to df4.\n",
    "    Analysis of plan length for which in ['double', 'single']:\n",
    "    PlanLength is double(single)-digit.\n",
    "    \"\"\"\n",
    "    if which == 'double':\n",
    "        msk = dfa.PlanLength >= 10\n",
    "        col2 = 'Frequency where PlanLength >=10'\n",
    "    else:\n",
    "        msk = dfa.PlanLength < 10\n",
    "        col2 = 'Frequency where PlanLength <10'\n",
    "        \n",
    "    dfa_rows = dfa.shape[0]\n",
    "    \n",
    "    dfout = dfa[msk].sort_values(['PlanLength'], ascending=False)\n",
    "\n",
    "    uniq_probs = dfout['Air cargo problem'].unique()\n",
    "    n_plans = dfout.shape[0]\n",
    "    searcher_cnt = dfout['Searcher'].value_counts()\n",
    "    fn_cnt = dfout['search_fn'].value_counts()\n",
    "\n",
    "    # get the html string:\n",
    "    df_fn = fn_cnt.to_frame()\n",
    "    df_fn.reset_index(drop=False, inplace=True)\n",
    "    df_fn.columns = ['Search function', col2]\n",
    "    \n",
    "    df_fn_html = df_fn.to_html(index=False, justify='center')\n",
    "    replace_str1 = ' style=\"text-align: center;\"'\n",
    "    replace_str2 = 'class=\"dataframe\"'\n",
    "    df_fn_html = df_fn_html.replace(replace_str1, '')\n",
    "    df_fn_html = df_fn_html.replace(replace_str2, replace_str1)\n",
    "\n",
    "    pct_plans = n_plans/dfa_rows\n",
    "    top2_fn = fn_cnt[0:2].sum()\n",
    "    pct_top2_fn = top2_fn/n_plans\n",
    "\n",
    "    text = f\"Out of {dfa_rows} completed searches, {pct_plans:.0%} ({n_plans}), have {which}-digit or longer PlanLength.<br>\"\n",
    "    text += f\"In that subset, {top2_fn:d} ({pct_top2_fn:.0%}) involve the search functions `{fn_cnt.index[0]}` and `{fn_cnt.index[1]}`.\"\n",
    "    if len(uniq_probs) < 4:\n",
    "        text += \" And this occurs only for Problems: \"\n",
    "        pro = \",\".join('{}' for p in uniq_probs) +'.<br>'\n",
    "        text += pro.format(*uniq_probs)\n",
    "    else:\n",
    "        text += \" And this occurs for all Problems.\"\n",
    "    text += \"<br>\"\n",
    "    \n",
    "    return df_fn_html, text, dfout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
